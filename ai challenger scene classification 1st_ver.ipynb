{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Building Network\n",
    "[Step 1](#step1): Unzip image dataset and check out train, validation and test files.\n",
    "\n",
    "[Step 2](#step2): Show image and json files in train & validation dataset.\n",
    "\n",
    "[Step 3](#step3): Initialize features(input) and labels(output) from images and json list.\n",
    "\n",
    "- read images from train/validation/test path.\n",
    "- read labels from train/validation json file.\n",
    "- resize and normalize images.\n",
    "- get batch and return feature_batch and label_batch.\n",
    "\n",
    "[Step 4](#step4): Build convolutional network, return training accuracy and training loss.\n",
    "\n",
    "[Step 5](#step5): Train on steps = 100.\n",
    "\n",
    "[Step 6](#step6): Train on full dataset.\n",
    "- epoch x, batch x, training loss, validation accuracy, evluation accuracy\n",
    "\n",
    "[Step 7](#step7): Test and write json submit file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Unzip image dataset and check out train, validation and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found extraced dataset\n"
     ]
    }
   ],
   "source": [
    "import os, zipfile\n",
    "\n",
    "train_path = 'E:/ai_challenger/scene classification/dataset/ai_challenger_scene_train_20170904.zip'\n",
    "validation_path = 'E:/ai_challenger/scene classification/dataset/ai_challenger_scene_validation_20170908.zip'\n",
    "test_a_path = 'E:/ai_challenger/scene classification/dataset/ai_challenger_scene_test_a_20170922.zip'\n",
    "extract_path = 'E:/ai_challenger/scene classification/dataset'\n",
    "\n",
    "def unzip(zipfile_path, extract_path, zipfile_name):\n",
    "    zipfile = zipfile.ZipFile(zipfile_path, 'r')\n",
    "    print('Extracting {} ...'.format(zipfile_name))\n",
    "    zipfile.extractall()\n",
    "    zipfile.close()\n",
    "    print('{} has been extracted.'.format(zipfle_name))\n",
    "\n",
    "if os.path.exists(extract_path):\n",
    "    print('Found extraced dataset')\n",
    "else:\n",
    "    unzip(train_path, extract_path, 'training dataset')\n",
    "    unzip(validation_path, extract_path, 'validation dataset')\n",
    "    unzip(test_a_path, extract_path, 'test dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Show image and json files in train & validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_url': 'https://n1-q.mafengwo.net/s1/M00/6B/72/wKgBm04Wc5WzFXU0AAHf09bdpiY84.jpeg?imageView2%2F2%2Fw%2F600%2Fq%2F90', 'label_id': '66', 'image_id': '79f993ae0858ae238b22968c5934d1ddba585ae4.jpg'}, {'image_url': 'http://news.sogou.com/', 'label_id': '61', 'image_id': 'e963208fe9e90df0c385f7367bcdb6d0d5d0b165.jpg'}, {'image_url': 'http://img2.fawan.com/2016/12/30/e967f93e7713c57cd2b00b832dd6091a_500x-_90.jpg', 'label_id': '64', 'image_id': '02df5ecbf7c749ccc9d833f129bbd5d9837940ce.jpg'}, {'image_url': 'https://b1-q.mafengwo.net/s1/M00/F2/C9/wKgBm04Wx3a-gk2FAAKbPKX7E9w91.jpeg?imageView2%2F2%2Fw%2F600%2Fq%2F90', 'label_id': '31', 'image_id': '5620eb385b7567fb087813cf5233b5ceecdeeca3.jpg'}, {'image_url': 'http://news.sogou.com/', 'label_id': '19', 'image_id': 'f8b4d42001a562fc63b9b39c02531661c0e236ca.jpg'}, {'image_url': 'http://www.user2.jqw.com/2014/01/06/1347666/product/b201401072000291460.JPG', 'label_id': '11', 'image_id': '57e7eb438670a4519041dab1482f2594a92f8a09.jpg'}, {'image_url': 'http://s16.sinaimg.cn/middle/67bde22dx929ff224e80f&690', 'label_id': '22', 'image_id': 'addb2ef7e4aa1a160093e32ceec19bf900c05d2e.jpg'}, {'image_url': 'http://imgsrc.baidu.com/imgad/pic/item/a686c9177f3e6709f65104d631c79f3df8dc5541.jpg', 'label_id': '11', 'image_id': '84a5b79a7f8fe3ddb43355eaf010a3a432e457b4.jpg'}, {'image_url': 'http://news.sogou.com/', 'label_id': '47', 'image_id': '48f690ba20db3e6a0a0f7ab5b59480f7558b18fa.jpg'}, {'image_url': 'http://news.sogou.com/', 'label_id': '3', 'image_id': '3c53b82532f132da2727fad84ade044f364a1dba.jpg'}]\n",
      "\n",
      "\n",
      "53879\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "import numpy as np\n",
    "\n",
    "train_features_path = r'E:\\ai_challenger\\scene classification\\dataset\\ai_challenger_scene_train_20170904\\scene_train_images_20170904'\n",
    "train_labels_path = r'E:\\ai_challenger\\scene classification\\dataset\\ai_challenger_scene_train_20170904\\scene_train_annotations_20170904.json'\n",
    "validation_features_path =r'E:\\ai_challenger\\scene classification\\dataset\\ai_challenger_scene_validation_20170908\\scene_validation_images_20170908'\n",
    "validation_labels_path =r'E:\\ai_challenger\\scene classification\\dataset\\ai_challenger_scene_validation_20170908\\scene_validation_images_20170908\\scene_validation_annotations_20170908.json'\n",
    "test_a_features_path = r'E:\\ai_challenger\\scene classification\\dataset\\ai_challenger_scene_test_a_20170922\\scene_test_a_images_20170922'\n",
    "\n",
    "# Show train label list\n",
    "with open(train_labels_path, 'r') as f:\n",
    "    train_label_list = json.load(f)\n",
    "    print(train_label_list[:10])\n",
    "    train_dict = {}\n",
    "    for image in train_label_list:\n",
    "        train_dict[image['image_id']] = int(image['label_id'])\n",
    "    print('\\n')\n",
    "    print(len(train_dict))     \n",
    "\n",
    "# Show train features list resulting out of memory..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Initialize features(input) and labels(output) from images and json list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from scipy.misc import imread, imresize\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class initialize(object):\n",
    "    # Get image-label list for train and validation\n",
    "    def __init__(self, feature_path, label_path):\n",
    "        self.image_label_dict = {}\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_list = json.load(f)\n",
    "        for image in label_list:\n",
    "            self.image_label_dict[image['image_id']] = int(image['label_id'])\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        self.length = len(self.image_label_dict) # number of feature images\n",
    "        self.image_name = list(self.image_label_dict.keys())\n",
    "        self.feature_path = feature_path\n",
    "    \n",
    "    # Read image in feature path, resize and normalize to [-1, 1]\n",
    "    def get_image(self, image_path, image_size):\n",
    "        image = imread(image_path)\n",
    "        image = imresize(image, [image_size, image_size])       \n",
    "        image = np.array(image).astype(np.float32)\n",
    "        image = 2 * (image - np.min(image)) / np.ptp(image) - 1\n",
    "        return image\n",
    "    \n",
    "    # Get feature and label batch\n",
    "    def get_batch(self, batch_size, image_size):\n",
    "        self.start = self.end\n",
    "        if self.start >= self.length:\n",
    "            self.start = 0\n",
    "        batch_feature = []\n",
    "        batch_label = []\n",
    "        index = self.start\n",
    "\n",
    "        while len(batch_feature) < batch_size:\n",
    "            i_image_path = os.path.join(self.feature_path, self.image_name[index])\n",
    "            i_image = self.get_image(i_image_path, image_size)\n",
    "            i_label = self.image_label_dict[self.image_name[index]]\n",
    "\n",
    "            batch_feature.append(i_image)\n",
    "            batch_label.append(i_label)\n",
    "            index += 1\n",
    "        self.end = index\n",
    "        return batch_feature, batch_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4: Build convolutional network, return training accuracy and training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_network(feature, label, num_class, image_size, keep_prob):\n",
    "    # Input layer\n",
    "    input_layer = tf.reshape(feature, [-1, image_size, image_size, 3])\n",
    "\n",
    "    # Conv1 \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer, \n",
    "        filters=32, \n",
    "        kernel_size=3,\n",
    "        strides=1, \n",
    "        padding='same', \n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.truncated_normal_initializer()\n",
    "        )\n",
    "    # Batch normalization 1\n",
    "    bn1 = tf.layers.batch_normalization(conv1)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=bn1, pool_size=[2, 2], strides=2)\n",
    "    # Max pooling 1: [-1, image_size/2, image_size/2, 32]\n",
    "\n",
    "    # Conv2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1, \n",
    "        filters=64, \n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding='same', \n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.truncated_normal_initializer()\n",
    "        )\n",
    "    # Batch normalization 2\n",
    "    bn2 = tf.layers.batch_normalization(conv2) \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=bn2, pool_size=[2, 2], strides=2)\n",
    "    # Max pooling 2: [-1, image_size/4, image_size/4, 64]\n",
    "\n",
    "    # Conv3\n",
    "    conv3 = tf.layers.conv2d(\n",
    "        inputs=pool2, \n",
    "        filters=128, \n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding='same', \n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.truncated_normal_initializer()\n",
    "        )\n",
    "    # Batch normalization 2\n",
    "    bn3 = tf.layers.batch_normalization(conv3)\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=bn3, pool_size=[2, 2], strides=2)\n",
    "    # Max pooling 2: [-1, image_size/8, image_size/8, 128]\n",
    "\n",
    "    # Flatten layer\n",
    "    flatten = tf.reshape(pool3, [-1, image_size * image_size * 2]) \n",
    "\n",
    "    # Fully connected layer\n",
    "    dense = tf.layers.dense(inputs=flatten, units=1024)\n",
    "    dropout = tf.nn.dropout(dense, keep_prob) # or tf.layers.dropout(inputs, rate)\n",
    "\n",
    "    # Output layer: returns logits and predictions\n",
    "    logits = tf.layers.dense(dropout, units=num_class) \n",
    "    output = tf.sigmoid(logits)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    return train_opt, cost, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='step5'></a>\n",
    "## Step 5: Train on steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Training Accuracy 0.062... Training Loss 4287.052...\n",
      "Step 11 Training Accuracy 0.016... Training Loss 5484.894...\n",
      "Step 21 Training Accuracy 0.062... Training Loss 2701.367...\n",
      "Step 31 Training Accuracy 0.078... Training Loss 1672.494...\n",
      "Step 41 Training Accuracy 0.047... Training Loss 1068.504...\n",
      "Step 51 Training Accuracy 0.047... Training Loss 773.654...\n",
      "Step 61 Training Accuracy 0.062... Training Loss 697.697...\n",
      "Step 71 Training Accuracy 0.047... Training Loss 583.394...\n",
      "Step 81 Training Accuracy 0.172... Training Loss 459.619...\n",
      "Step 91 Training Accuracy 0.141... Training Loss 415.706...\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train(train_feature_path, train_label_path, num_class, batch_size, image_size, train_steps):\n",
    "    train = initialize(train_feature_path, train_label_path)\n",
    "        \n",
    "    feature = tf.placeholder(tf.float32, shape=[None, image_size, image_size, 3], name='feature')\n",
    "    label = tf.placeholder(tf.float32, shape=[None], name='label')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    one_hot_label = tf.one_hot(indices=tf.cast(label, tf.int32), depth=80)\n",
    "    train_opt, cost, logits = conv_network(feature, one_hot_label, num_class, image_size, keep_prob)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float32'))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for steps in range(train_steps):\n",
    "            train_feature_batch, train_label_batch = train.get_batch(batch_size, image_size)\n",
    "            sess.run(train_opt, feed_dict={feature: train_feature_batch, label: train_label_batch, keep_prob: 0.5})\n",
    "                \n",
    "            if steps % 10 == 0:\n",
    "                train_accuracy = sess.run(accuracy, feed_dict={feature: train_feature_batch, label: train_label_batch, keep_prob: 0.5})\n",
    "                train_loss = sess.run(cost, feed_dict={feature: train_feature_batch, label: train_label_batch, keep_prob: 0.5})\n",
    "                print('Step {}'.format(steps+1),\n",
    "                      'Training Accuracy {:.3f}...'.format(train_accuracy),\n",
    "                      'Training Loss {:.3f}...'.format(train_loss),\n",
    "                     ) \n",
    "        print('Training completed')\n",
    "\n",
    "# Train on 100 steps:\n",
    "train_feature_path = r'E:\\ai_challenger\\scene classification\\dataset\\ai_challenger_scene_train_20170904\\scene_train_images_20170904'\n",
    "train_label_path = r'E:\\ai_challenger\\scene classification\\dataset\\ai_challenger_scene_train_20170904\\scene_train_annotations_20170904.json'\n",
    "num_class = 80\n",
    "image_size = 32\n",
    "batch_size = 64\n",
    "learning_rate =1e-3\n",
    "train_steps = 100\n",
    "train(train_feature_path, train_label_path, num_class, batch_size, image_size, train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
